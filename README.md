## Intro
Gesture-based interfaces are systems that allow users to interact with them by using hand or other body components. These applications are getting more and more popular and have a wide range of use cases; for example in [home automation, healthcare, automative, virtual reality](https://emerj.com/ai-sector-overviews/artificial-intelligence-in-gestural-interfaces/), etc.  If well designed, gesture-based interfaces feel more natural, intuitive and easier to learn.    
The goal of this project is to use an Artificial Neural Network to recognise a set of hand gestures and use those to interact with a YouTube player.  Why Youtube? Well, it's popular, there is no need to install a software locally, you can find pretty much any visual content and it's free (as long as you're ok with advertisement ;). But of course, you can implement the same technique to control a local media player; just make sure the application allows for keyboard shortcuts.

## What interactions
The interactions that were implemented are:  
* simple virtual mouse functionalities (move, left and right click);
* play and pause the video;
* increase and decrease the volume of both the media player and computer;
* forward and backward jumps;
* activate and deactivate fullscreen mode;
* activate and deactivate subtitles/ closed captions, when these are available;
* additionally, if user is sleeping or has left, automatically pause the video.  

In the images below you can see an example of the aforementioned interactions.
<img src="readme_images/detection_1.png" height = 400 px>
<img src="readme_images/detection_2.png" height = 400 px>  

You can find a demo of the project [here](https://youtu.be/gHVrGI3632s) !

All feedbacks on what can be improved are welcome :)  
